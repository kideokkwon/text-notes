{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE3RUkFftAHDyL0p/Ct2zC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# (Rathje, Katila and Reineke, 2024) Making the most of AI and machine learning in organizations and strategy research: Supervised machine learning, causal inference, and matching models\n",
        "\n",
        "[Link](https://onlinelibrary.wiley.com/doi/full/10.1002/smj.3604)"
      ],
      "metadata": {
        "id": "bamIBbwtICyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "Guide researchers in the use of machine learning approaches to choosing matching variables for enhanced causal inference in propensity score matching models."
      ],
      "metadata": {
        "id": "79Cy1VfaIC0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Our contribution is to highlight the use of supervised ML in two-stage matching models to deal with sample selection bias (Heckman & Todd, 2009). In particular, we demonstrate the use of ML to determine variables on which to match in the first-stage model.\n",
        "\n",
        "ML can play a role in the pretreatment covariate selection process. It can be used in place of or to augment, ad hoc selection of first-stage variables, including pretreatment and confounding covariates. By improving the first-stage, second-stage estimates of causal effects can potentially be made more precise and more reproducible (Angrist & Frandsen, 2022). ML can thus improve robustness of second-stage parameter estimates by acting as a data preprocessing method.\n",
        "\n",
        "Manually including all pairwise interactions and complex functional forms is often infeasible and computatonially not possible, so researchers pick a subset. In contrast, ML sorts through these interactions and functional forms automatically.\n",
        "\n",
        "Boundaries (limitations) of this method:\n",
        "- supervised ML methods do not provide standard errors, so no coefficient estimates can be readily interpreted (note: why is this an issue though?)\n",
        "- cannot relinquish our responsibility as researchers to a \"machine\". Human interpretation still plays a crucial role, as in the initial set of confounding variables that are originally included need to be selected by the researcher.\n",
        "- confounders automatically selected for the prediction in first stage do not alawys match the researcher reasoning because the code picks \"randomly\" of highly multicollinear variables (although researcher can require a particular variable to be included).\n",
        "- carry the risk of introducing more uncertainty rather than reducing it."
      ],
      "metadata": {
        "id": "b7ww31TzIRz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Prediction Tasks in Strategy: Prior Work\n",
        "\n",
        "## 2.1.1 Prior PSM papers in strategy research\n",
        "\n",
        "Only about 3% of papers reviewed tested for different functional forms of the varaibles in the first stage, and just about 30% of the papers were sensitive to explaining why particular variables were included in the first stage.\n",
        "\n",
        "## 2.1.2 How ML can help\n",
        "\n",
        "particularly regularization and cross-validation are particularly relevant.\n",
        "\n"
      ],
      "metadata": {
        "id": "_B3I4ODsIC2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Empirical Demonstration: The Case of Two-Stage Matching Models and ML\n",
        "\n",
        "## 3.1 Data sources and variables\n",
        "\n",
        "## 3.2 Looking back: Traditional approaches\n",
        "\n",
        "Used very few covariates\n",
        "\n",
        "### 3.2.1 Possible shortfalls of traditional approaches\n",
        "\n",
        "A limitation is that it is often not possible to know which confounders to add. What scholars with traditional methods cannot provide is a numerical evaluation of this step. As we describe in more detail below, ML offers a quantifiable step in this direction.\n",
        "\n",
        "## 3.3 Moving forward: Steps of including ML\n",
        "\n",
        "### 3.3.1 How to apply ML\n",
        "\n",
        "1. split data: ML approach starts by splitting the data into subsets: training, validation, and test.\n",
        "2. select an ML model, add regularization, and cross-validate: first, we select a supervised learning method for our predictive model. In the case of PSM, we often use logistic regression. Logistic regression is useful because it generates a continuous probability of an observation being treated by regressing the observation's set of covariates $(X_i)$ on its observed treatment $(Y_i)$. As a result, logistic regression model (as opposed to other ML techniques such as SVM) allows us to generate continuous estimates of propensity scores.\n",
        "\n",
        "ML aims to decrease subjectivity in the covariate selection process and prevent overfitting.\n",
        "\n",
        "*Regularization* suggests covariates important for prediction. Regularization starts with a list of variables that the researcher has compiled and then prunes down the list, limiting model complexity. A standard way to regularize is to penalize complex models in favor of simpler models. Most common forms for regularization includes Lasso (L1), Ridge (L2) and Elastic Net.\n",
        "\n",
        "*Cross-Validation* allows scholars to validate the quality of the prediction by quantitatively testing how well the results generalize to yet unseen test data.\n",
        "\n",
        "With the final model (i.e., after regularization and cross-validation is complete), we can investigate the coefficient weights (sizes of $\\theta$) to determine which covariates contributed to treatment selection bias. Those covariates with the largest coefficient weights in magnitude contribute the most to treatment selection bias. As a result, higher weights indicate the need to include the corresponding covariates in the matching model.\n",
        "\n",
        "3. Repeat by adding interactions. Then, we can repeat the model by adding more covariates (adding quadratic interactions, etc.,). To protect against overfitting, we evaluate the performance of each model independently.\n",
        "\n",
        "4. Evaluate and select the ML-PSM model. Once all the models are created, the next step is to compare their performance. An effective combination is to measure predictive performance using AUC and the model fit using $R^2$. As noted above, because AUC is the measure of predictiveness, supervised ML prioritizes it as the principal measurement of model selection. Simply put, AUC is a ratio of TPR (TP / (TP + FN), or correct truths divided by all that were classified as truths) and FPR (FP / (FP + TN), or those classified as true out of all falses). $R^2$ allows us to interpret the impact of increasing model complexity on fit. For example, as model complexity grows (e.g., adding more interactions), if AUC begins to decrease while $R^2$ increases, the model is likely overfitting. Other classification metrics such as precision, recall, and accuracy can also be considered.\n",
        "\n",
        "5. Match on propensity scores and plot the results.\n",
        "\n",
        "6. Inspect and compare second-stage regressions.\n",
        "\n"
      ],
      "metadata": {
        "id": "fP2GoPH9TSvZ"
      }
    }
  ]
}