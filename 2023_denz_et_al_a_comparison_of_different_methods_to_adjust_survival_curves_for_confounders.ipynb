{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpfpdG8pIgDxtknrYbAYaC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# (Denz et al., 2023) A comparison of different methods to adjust survival curves for confounders"
      ],
      "metadata": {
        "id": "Lchu9zSmAwcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract"
      ],
      "metadata": {
        "id": "dqdF_9C0Awe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple methods to adjust survival curves for confounders exist, but it is currently unclear which method is the most appropriate in which situation. Our goal is to compare forms of\n",
        "- inverse probability of treatment weighting\n",
        "- the $g$-formula\n",
        "- propensity score matching\n",
        "- empirical likelihood estimation\n",
        "- augmented estimators as well as their pseudo-values based counterparts\n",
        "\n",
        "in different scenarios with a focus on their bias and goodness-of-fit.\n",
        "\n",
        "When used properly, all methods showed no systematic bias in medium to large samples. Cox regression based methods, however, showed systemic bias in small samples.\n",
        "\n",
        "The goodness-of-fit varied greatly between different methods and scenarios. Methods utilizing an outcome model were more efficient than other techniques, while augmented estimators using an additional treatment assignment model were unbiased when either model was correct with a goodness-of-fit comparable to other methods. These \"doubly-robust\" methods have important advantages in every considered scenario.\n",
        "\n",
        "The methods were illustrated by contrasting the survival of smokers and non-smokers, using data from the German Epidemiological Trial on Ankle-Brachial-Index. Subsequently, we compare the methods using a Monte-Carlo simulation. We consider scenarios in which correctly or incorrectly specified models for describing the treatment assignment and the time-to-event outcomes are used with varying sample sizes. The bias and goodness-of-fit is determined by taking the entire survival curve into account."
      ],
      "metadata": {
        "id": "kiZbtmfUAwhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "CG0a72sfBt0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the analysis of clinical time-to-event data, treatment-specific survival curves are often used to graphically display the treatment effect in some population. The KP estimator, stratified by treatment allocation is usually used to calculate these curves. Simple KP estimates do not take confounders into account.\n",
        "\n",
        "The most popular way to adjust for confounders in medical time-to-event analysis is the use of the Cox proportional hazards model. Many researchers report both adjusted hazard-ratios (from Cox) and unadjsuted KP estimates. Since the latter does not correct for the presence of confounders, these results often differ and hence confuse the reader.\n",
        "\n",
        "Confounder-adjusted survival curves are a solution to this problem. Various methods for calculating these have been developed, but their properties have only been studied to a limited extent.\n",
        "\n",
        "Article structure:\n",
        "1. Formal description of confounder-adjusted survival curves and the background\n",
        "2. Brief description of all included methods. Using real data, illustrate the usage of these methods by comparing the survival of non-smokers and current or past smokers.\n",
        "3. The design of the simulation study is described and the results are presented.\n",
        "4. Discuss the results and their implications for the practical applications of the adjustment methods.\n"
      ],
      "metadata": {
        "id": "M9FA9gtABt3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Background and Notation"
      ],
      "metadata": {
        "id": "IHtlQy6CBt5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $Z\\in\\{0,1,...,k\\}$ denote the treatment group, where each value of $Z$ indicates one of $k$ possible treatments. Let $T$ be the time to the occurrence of the event of interest. In reality, it is sometimes only known whether a person has suffered an event by time $C$ or not, which is known as right-censored data. In this case, only $T_{obs}=\\min(T,C)$ would be observed with a corresponding event indicator $D=I(T < C)$. Although a crucial point for estimation methods, it is unimportant for thd definition of the target estimand.\n",
        "\n",
        "Under the Neyman-Rubin causal framework, every person has $k$ potential survival times $T^z\\in\\{T^0, T^1,...,T^k\\}$, one for each of the $k$ possible treatment strategies. The goal is to estimate the counterfactual survival probability in the target population over time, where every person has received the same treatment. This population consists of $N$ individuals, indexed by $i$, $i=1,2,...,N$, each with their own vector of baseline covariates $x_i$.\n",
        "\n",
        "The counterfactual survival probability of individual $i$ at time $t$ is defined by:\n",
        "\n",
        "$$S(t|Z=z,X=x_i)=P(T^z > t|x_i)$$\n",
        "\n",
        "where $T^z$ denotes the failure time which would have been observed, if $Z=z$ was actually administered. Therefore, the *target* function is defined as:\n",
        "\n",
        "$$S_z(t)=E(I(T^z > t))$$\n",
        "\n",
        "In the literature, this quantity is often called:\n",
        "- causal survival curve\n",
        "- counterfactual survival curve\n",
        "- confounder-adjusted survival curve\n",
        "\n",
        "The difference or ratio between two treatment-specific counterfactual survival curves is sometimes used to define the *average treatment effect*.\n",
        "\n",
        "To estiamte such an effect (without randomization), 3 assumptions are needed:\n",
        "- SUTVA\n",
        "- no unmeasured confounders\n",
        "- positivity\n",
        "\n"
      ],
      "metadata": {
        "id": "KUglYNt0SJ_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Overview of Methods"
      ],
      "metadata": {
        "id": "WBW1DmxZSKBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Focus strictly on methods that can be used to adjust survival curves for measured baseline confounders when random right-censoring is present.\n",
        "\n",
        "The article claims that methods that are concerned with:\n",
        "- covariate adjustment to increase power\n",
        "- corrections for covariate-dependent censoring\n",
        "- time-varying confounding\n",
        "- unmeasured confounding\n",
        "\n",
        "are disregarded.\n",
        "\n",
        "The *average covariate* method, which entails fitting a cox model to the data and plugging in the mean of all covariates in order to predict the survival probability for each treatment at a range of time points is also disregarded.\n",
        "\n",
        "Multiple methods based on stratification are also excluded because they are only defined for categorical confounders.\n",
        "\n",
        "Also excluded *Target Maximum Likelihood Estimation* based methods because they currently only work for discrete-time survival data.\n",
        "\n",
        "This leaves us with:\n",
        "- G-formula\n",
        "- IPTW\n",
        "- PSM\n",
        "- Empirical likelihood estimation\n",
        "- AIPTW and their pseudo values based counterparts\n",
        "\n",
        "Split into 3 categories:\n",
        "- methods utilizing the outcome mechanism\n",
        "- methods that use the treatment assignment mechanism\n",
        "- methods that rely on both\n",
        "\n"
      ],
      "metadata": {
        "id": "ciJ1x_X_Wnwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 G-Formula"
      ],
      "metadata": {
        "id": "6DLs0TGoWnzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the confounders are adjusted for by correctly modeling the outcome mechanism. The Cox prop. model in conjunction with an estimate of the baseline-hazard function is used (usually).\n",
        "\n",
        "Proposed by Makuch (1982) and Chang et al (1982) using a simple Cox model."
      ],
      "metadata": {
        "id": "YRHiUxVNeoMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Inverse probability of treatment weighting (IPTW)"
      ],
      "metadata": {
        "id": "PFJUPJt0hK4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "propensity scores are estimated for each individual and each treatment. using the weights of the inverse, confounding is removed.\n",
        "\n",
        "for survival analysis, there is the IPTW HZ estimator by Cole and Hernan (2004), equivalent to fitting a weighted stratified Cox model, using the treatment indicator as stratification variable.\n",
        "\n",
        "Xie and Liu (2005) propose a directly weighted KP estimator (IPTW KM)"
      ],
      "metadata": {
        "id": "VHfmG4UyhLAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Propensity Score Matching"
      ],
      "metadata": {
        "id": "2zjh79sxeoOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of analysis of time-to-event data, PSM has been shown to be less efficient than IPTW and G-formula."
      ],
      "metadata": {
        "id": "31rN_cs1hxK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Augmented Inverse Probability of Treatment Weighting (AIPTW)"
      ],
      "metadata": {
        "id": "akgYbnD_hxNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Works by using the G-formula estimate to augment the IPTW estimate in order to make it more efficient. Essentially, it is just the IPTW estimator with the conditional survival predictions under each treatment added to it, after weightnig them using the propensity score. Its feature of \"doubly-robust\" is the main advantage of this method."
      ],
      "metadata": {
        "id": "6sZdGXOeiGbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 G-Formula + IPTW"
      ],
      "metadata": {
        "id": "3ElPNIZuisFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proposed by Chatton et al (2021).\n",
        "\n",
        "Works by first the IPTW are estimated. Then the weights are used in the estimation of the outcome model. The outcome model should also include the relevant confounders as covariates.\n",
        "\n",
        "This model can then be used to calculate standard G-formula estimates as described earlier. Often paired with a Cox model in the G-formula.\n",
        "\n"
      ],
      "metadata": {
        "id": "IN2B1z3wiGdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Empirical likelihood estimation (EL)"
      ],
      "metadata": {
        "id": "81goEwg2iuIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wang et al (2019) proposed an estimator based on the empirical likelihood estimation methodology. It is a model-free approach that works by forcing the moments of the covariates $X$ to be equal between treatment groups, through the maximization of a contrained likelihood function. Simulation studies indicate this method shares the doubly-robust property and can outpoerform IPTW in terms of variance in some scenarios."
      ],
      "metadata": {
        "id": "lnqBbsBNiuKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Discussion"
      ],
      "metadata": {
        "id": "wTuCUmFDjUBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results of this study and previous discussion on the topic, we recommend using AIPTW based methods, because they possess the doubly-robust property and showed goodness-of-fit similar to IPTW based methods. Although the G-formula and G-formula PV methods showed better goodness-of-fit overall, they do rely on one correctly specified model. The drawback of AIPTW is that they are more complex to use than other methods, because an implementation of the method itself and of isotonic regression is required. This problem is mitigated by the user-friendly R-code implementations in the riskRegression and adjustedCurves packages."
      ],
      "metadata": {
        "id": "90bgyikAmWmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZW8CZwHZmWof"
      }
    }
  ]
}