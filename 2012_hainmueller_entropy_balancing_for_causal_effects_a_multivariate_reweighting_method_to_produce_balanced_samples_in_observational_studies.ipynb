{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5apDXAR9wy1b/gATv1Z/C"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# (Hainmueller, 2012) Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies"
      ],
      "metadata": {
        "id": "nN8bEzu5rQCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This paper proposes *entropy balancing*, a data preprocessing method to achieve covariate balance in observational studies with binary treatments.\n",
        "\n",
        "Entropy balancing relies on a maximum entropy reweighting scheme that calibrates unit weights so that the reweighted treatment and control group satisfy a potentially large set of prespecified balance conditions that incorporate information about known sample moments (first, second, and possibly higher moments).\n",
        "\n",
        "These balance improvements can reduce model dependence for the subsequent estimation of treatment effects. It also obviates the need for continual balance checking and iterative searching over propensity score models that mayu stochastically balance the covariate moments."
      ],
      "metadata": {
        "id": "t5oTpGQwrQFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction"
      ],
      "metadata": {
        "id": "UCE-Y4qUshah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important concern is that many commonly used preprocessing approaches do not directly focus on the goal of producing covariate balance.\n",
        "\n",
        "In contrast, entropy balancing involves a reweighting scheme that directly incorporates covariate balance into the weight function that is applied to the sample units.\n",
        "\n",
        "The author claims that there are 4 advantages to using entropy balancing:\n",
        "1. Most importantly, allows high degree of covariate balance by balancing first, second, and possibly higher moments of the covariate distributions as well as interactions.\n",
        "2. Retains valuable information in the preprocessed data by allowing the unit weights to vary smoothly across units, in contrast to methods like NNM where units are discarded.\n",
        "3. Fairly versatile. The resulting weights can be used for a simple weighted difference in means, a weighted OLS, etc.\n",
        "4. Computationally attractive since the optimization problem to find the unit weights is well behaved and globally convex.\n",
        "\n",
        "This paper borrows methods from survey, moments estimation, empirical likelihood, exponential tilting, and missing data literature."
      ],
      "metadata": {
        "id": "_cMMKnezshdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Observational Study with Binary Treatments"
      ],
      "metadata": {
        "id": "CoS8sIYVshfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Framework"
      ],
      "metadata": {
        "id": "IBCOdjOA4TP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A brief on the potential outcomes framework."
      ],
      "metadata": {
        "id": "sWKUSze24TSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Achieving Balance with Matching and Propensity Score Methods"
      ],
      "metadata": {
        "id": "Dt5tTlA-4TUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The practice of matching, assessing balance, and redo'ing is sometimes referred to as the \"propensity score tautology\" and has been criticized. This iterative process can be tedious and frequently results in low balance levels.\n",
        "\n",
        "One way to improve the search for a better balancing score is to replace the logistic regression with a better estimation technique for the assignment mechanism such as boosted regression or kernel regression. Entropy balancing takes a different approach and directly focuses on covariate balance."
      ],
      "metadata": {
        "id": "Dyile1Dk4TXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Entropy Balancing"
      ],
      "metadata": {
        "id": "wfusjfh9S6VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A preprocessing procedure that allows researchers to create balanced samples for the subsequent estimation of treatment effects. The balance constraints ensure that the reweighted groups match exactly on the specified moments."
      ],
      "metadata": {
        "id": "RrFBNm4YS6XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Entropy Balancing Scheme"
      ],
      "metadata": {
        "id": "pxizUsGfhgrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience, supose that the researcher's goal is to reweight the control group to match the moments of the treatment group in order to subsequently estimate\n",
        "\n",
        "$$\\text{PATT}=\\tau=E[Y(1)|D=1]-E[Y(0)|D=1]$$\n",
        "\n",
        "using the difference in mean outcomes between the treatment group and the reweighted control group. In this case, the counterfactual mean may be estimated by\n",
        "\n",
        "$$\\hat{E}[Y(0)|D=1]=\\frac{\\sum_{\\{i|D=0\\}}Y_iw_i}{\\sum_{\\{i|D=0\\}}w_i}$$\n",
        "\n",
        "where $w_i$ is a weight chosen for each control unit. The weights are chosen by the following reweighting scheme\n",
        "\n",
        "$$\\min_{w_i}H(w)=\\sum_{\\{i|D=0\\}}h(w_i)$$\n",
        "\n",
        "subject to balance and normalizing constraints\n",
        "\n",
        "$$\\sum_{\\{i|D=0\\}}w_ic_{ri}(X_i)=m_r$$with $r\\in 1,...,R$\n",
        "\n",
        "$$\\sum_{\\{i|D=0\\}}w_i=1$$ and\n",
        "\n",
        "$$w_i > 0$$ for all $i$ such as $D=0$,\n",
        "\n",
        "where $h(\\cdot)$ is a distance metric and $c_{ri}(X_i)=m_r$ describes a set of $R$ balance constraints imposed on the covariate moments of the reweighted control group.\n",
        "\n",
        "The loss function $h(\\cdot)$ is a distance metric chosen from the general class of empirical minimum discrepancy estimators from Cressie 1988. The algorithm prefers Kullback (1959)'s *entropy* divergence, $h(w_i)=w_i\\log (w_i/q_i)$ with estimated weight $w_i$ and base weight $q_i$, and is presumely where the term \"entropy balance\" comes from.\n",
        "\n",
        "The entropy balancing scheme can be understood as a generalization of the conventional propensity score weighting approach where the researcher first estimates the unit weights with a logistic regression and then computes balance checks to see if the estimated weights indeed equalize the covariate distributions.\n",
        "\n",
        "In many empirical cases, we would expect the bulk of the confounding to depend on the first and second moments."
      ],
      "metadata": {
        "id": "3vavyjHghgtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Implementation"
      ],
      "metadata": {
        "id": "HgB6msPfhj_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some math regarding Lagrange multiplers that I did not take the time to understand."
      ],
      "metadata": {
        "id": "lZWuxtfvhkBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Alternative Base Weights"
      ],
      "metadata": {
        "id": "kt42PfEPq_wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "same"
      ],
      "metadata": {
        "id": "wELhImAWq_zG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Estimation in the Preprocessed Data"
      ],
      "metadata": {
        "id": "cI0ycloRrVVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outcome model can be any of the traditionally used models such as outcome regression. Note that the outcome model can further address the correlation between the outcome and covariates in the weighted data and also provide variance estimates for the treatment effects. In addition, such regression models may include covariates or interactions that are not directly incldued in the reweighting to remove bias that may arise from remaining differences between the treatment and the reweighted control group. The outcome model may also increase precision if the additional variables in the outcome model account for residual variation in the outcome of interest."
      ],
      "metadata": {
        "id": "5UNt83Knrcq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Entropy Balancing and Other Preprocessing Methods"
      ],
      "metadata": {
        "id": "-j3IbrkCrctI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy balancing share a similarity with genetic matching as it also directly focuses on covariate balance.\n",
        "\n",
        "Entropy balancing is also related to CEM as covariate balance is specified before the preprocessing adjustment, but entropy balancing also differs from CEM in important ways as CEM discards units while EB does not."
      ],
      "metadata": {
        "id": "_EgfrLZircwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Potential Limitations"
      ],
      "metadata": {
        "id": "A1lz_WensMzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "some issues with entropy balancing.\n",
        "\n",
        "If there exists no set of positive weights to satisfy the constraints, for example if the treatment group is 1% male but control is 99% male. Of-course, this challenge of finding good matches with limited overlap is shared by all matching methods.\n",
        "\n",
        "Also, while there may be a solution, due to limited overlap, the solution involves an extreme adjustment to the weights of some control units. As in the few control units may receive large weights because they contribute most information about the counterfactual of interest. Large weights increase the variance for the subsequent analysis. In this case, there may be weight refinement done to trim weigfhts that are considered too large."
      ],
      "metadata": {
        "id": "YFSq4nV2sM27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Weight Refinements"
      ],
      "metadata": {
        "id": "-LsD4SKOS6Zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this may be similar conceptually to stabilized weights in other weighting methods."
      ],
      "metadata": {
        "id": "gQw86lJOtzzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Monte Carlo Simulations"
      ],
      "metadata": {
        "id": "h7qpxtw_tz1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compares Naive Difference in means, PSM, MD, GM, PSMD, PSW, and EB (entropy balancing)."
      ],
      "metadata": {
        "id": "CmOXCq_4vYBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Design"
      ],
      "metadata": {
        "id": "kCGEolTqvYDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hainsmueller's simulation study showed that EB had the lowest root MSE for considering the 3 cases of:\n",
        "- equal variance\n",
        "- unequal variance\n",
        "- irrelevant covariates\n",
        "\n"
      ],
      "metadata": {
        "id": "9l3nYszjtz38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Results"
      ],
      "metadata": {
        "id": "qsQayxatv3n1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EB claims to have been best. apparently in larger samples, all methods did well."
      ],
      "metadata": {
        "id": "ftkqyBLXyYfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Empirical Applications"
      ],
      "metadata": {
        "id": "uMrLqsE5yYiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 The LaLonde Data"
      ],
      "metadata": {
        "id": "wQIw-Ea0v3qE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data set used very commonly as a canonical benchmark in the causal inference literature."
      ],
      "metadata": {
        "id": "Q27B4Xukyx-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 News Media Persuasion"
      ],
      "metadata": {
        "id": "3fRBTD4nyyAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a typical political science survey data"
      ],
      "metadata": {
        "id": "MX0LyGrey4Wd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Conclusion"
      ],
      "metadata": {
        "id": "GeGOTcaLy4Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While EB simplifies the search for covariate balance for practicioners, it is important to notice that other problems that are commonly associated with preprocessing methods still apply, such as the fact that it does not provide any safeguards against bias from unmeasured confounders, a vexing problem in observational studies."
      ],
      "metadata": {
        "id": "bXRlFoEvzM85"
      }
    }
  ]
}