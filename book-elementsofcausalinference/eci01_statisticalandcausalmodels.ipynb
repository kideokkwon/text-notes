{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEL7i6bYEo5Dmyf0qrkHj5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Elements of Causal Inference Chapter 1: Statistical and Causal Models"
      ],
      "metadata": {
        "id": "aFSB48hyGC8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Probability Theory and Statistics"
      ],
      "metadata": {
        "id": "OFiaBfMaGDB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probability theory and statistics are based on the model of a random experiment or probability space\n",
        "$$(\\Omega,\\mathcal{F},P)$$\n",
        "where:\n",
        "- $\\Omega$ is a set containing all possible outcomes\n",
        "- $\\mathcal{F}$ is a collection of events $A\\subseteq\\Omega$\n",
        "- $P$ is a measure assigning a probability to each event\n",
        "\n",
        "Probability theory allows us to reason about the outcomes of random experiments, given the preceding mathematical structure.\n",
        "\n",
        "Statistical learning deals with the inverse problem: We are given the outcomes of the experiments, and from this we want to infer properties of the underlying mathematical structure. For instance, suppose that we have observed data\n",
        "$$(x_1,y_1),...,(x_n,y_n)$$\n",
        "where:\n",
        "- $x_i\\in\\mathcal{X}$ are inputs (i.e., covariates)\n",
        "- $y_i\\in\\mathcal{Y}$ are outputs\n",
        "\n",
        "We may now assume that each $(x_i,y_i),i=1,...,n$ has been generated independently by the same unknown random experiment. More precisely, such a model assumes that the observations $(x_1,y_1),...,(x_n,y_n)$ are realizations of random variables $(X_1,Y_1),...,(X_n,Y_n)$ that are i.i.d with joint distribution $P_{X,Y}$. Here, $X,Y$ are random variables taking values in metric spaces $\\mathcal{X,Y}$. Almost *all* of statistics and machine learning builds on i.i.d data.\n",
        "\n",
        "We may now be interested in certain properties of $P_{X,Y}$, such as:\n",
        "- i. the expectation of the output given the input, $f(x)=\\mathbb{E}[Y|X=x]$, called regression, where often $\\mathcal{Y}=\\mathbb{R}$\n",
        "- ii. a binary classifier assigning each $x$ to the class that is more likely, $f(x)=\\arg\\max_{y\\in\\mathcal{Y}}P(Y=y|X=x)$, where $\\mathcal{Y}=\\{\\pm 1\\}$\n",
        "- iii. the density $p_{X,Y}$ of $P_{X,Y}$ (assuming it exists)\n",
        "\n",
        "In practice, we seek to estimate these properties from finite data sets, that is, based on the sample, or equivalently an empirical distribution $P_{X,Y}^n$ that puts a point mass of equal weight on each observation.\n",
        "\n",
        "This constitutes an **inverse problem**: We want to estimate a property of an object we cannot observe based on observations that are obtained by applying an operation (e.g., sampling from the unknown distribution) to the underlying object.\n",
        "\n"
      ],
      "metadata": {
        "id": "yNWn1KulHwKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Learning Theory"
      ],
      "metadata": {
        "id": "WH9PcU2GHwMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we discuss causal models, we will see that in a sense, the causal learning problem is harder in that it is ill-posed *on two levels*. In addition to the statistical ill-posed-ness, which is essentially because a finite sample of arbitrary size will never contain all iformation about the underlying distribution, there is an ill-posed-ness due to the fact that even complete knowledge of an observational distribution usually does not determine the underlying causal model.\n",
        "\n",
        "Let's look at the statistical learning problem in more detail, focusing on the case of **binary pattern recognition** or classification, where $\\mathcal{Y}=\\{\\pm 1\\}$.\n",
        "\n",
        "We seek to learn $f:\\mathcal{X}\\rightarrow\\mathcal{Y}$ based on observations $(x_1,y_1),...,(x_n,y_n)$, generated i.i.d from an unknown $P_{X,Y}$. Our goal is to minimize the expected error or **risk**\n",
        "\n",
        "$$R[f]=\\int\\frac{1}{2}|f(x)-y|dP_{X,Y}(x,y)$$ over some class of functions $\\mathcal{F}$. Note that this is an integral with respect to the measure $P_{X,Y}$; however, if $P_{X,Y}$ allows for a density $p(x,y)$ with respect to Lebesgue measure, the integral reduces to $\\int\\frac{1}{2}|f(x)-y|p(x,y)dxdy$.\n",
        "\n",
        "Of-course, since $P_{X,Y}$ is unknown, we cannot compute $R[f]$, let alone minimize it. Instead, we appeal to an **induction principle**, such as **empirical risk minimization**. We return the function minimizing the training error or empirical risk\n",
        "$$R_{\\text{emp}}^n[f]=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{1}{2}|f(x_i)-y_i|$$ over $f\\in\\mathcal{F}$.\n",
        "\n",
        "**Universal consistency** refers to convergence of a learning algorithm to the lowest achievable risk with *any* function. There are learning algorithms that are universally consistent, for instances nearest neighbor classifiers and Support Vector Machines. While universal consistency essentially tells us everything can be learned in the limit of infinite data, it does not imply that every problem is learnable well from finite data, due to the phenomenom of **slow rates**. For any learning algorithm, there exist problems for which the learning rates are arbitrarily slow. It does tell us, however, that if we fix the distribution, and gather enough data, then we can get arbitrarily close to the lowest risk eventually."
      ],
      "metadata": {
        "id": "12wu_6OgHwOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Causal Modeling and Learning"
      ],
      "metadata": {
        "id": "7LORdqHfHwRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causal modeling starts from another, arguably more fundamental, structure. A causal structure *entails* a probabilitymodel, but it contains additional information *not* contained in the latter.\n",
        "\n",
        "**Causal reasoning**, according to the terminology used in this book, denotes the process of drawing conclusions from a causal model, similar to the way probability theory allows us to reason about the outcomes of random experiments.\n",
        "\n",
        "We refer to the question of which parts of the causal structure can in principle be inferred from the joint distribution as **structure identifiability**. Unlike statistical learning, even full knowledge of $P$ does not make the solution trivial and needs additional assumptions.\n",
        "\n",
        "A well-known topos holds that *correlation does not imply causation*; in other words, statistical properties alone do not determine causal structures. It is less well known that one may postulate that while we cannot infer a concrete causal structure, we may at least infer the existence of causal links from statistical dependences.\n",
        "\n",
        "**Principle 1.1 (Reichenbach's common cause principle)**\n",
        "\n",
        "If two random variables $X$ and $Y$ are statistically dependent $(X\\not\\!\\perp\\!\\!\\!\\!\\perp Y)$, then there exists a third variable $Z$ that causally influences both. (As a special case, $Z$ may coincide with either $X$ or $Y$.) Furthermore, this variable $Z$ screens $X$ and $Y$ from each other in the sense that given $Z$, they become independent, $X\\perp\\!\\!\\!\\!\\perp Y|Z$.\n",
        "\n"
      ],
      "metadata": {
        "id": "jh06gXvuHwTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Two Examples"
      ],
      "metadata": {
        "id": "IrE3hNI22qHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goes over some examples of **structural causal models (SCMs)**, sometimes referred to as **structural equation models**. In an SCM, all dependencies are generated by functions that compute variables from other variables. In an SCM, all dependencies are generated by functions that compute variables from other variables. Crucially, these functions are to be read as assignments, that is, as functions as in computer science rather than as mathematical equations. We usually think of them as modeling physical mechanisms. An SCM entails a joint distribution over all observables. This book takes SCMs as our starting point and try to develop everything from there.\n",
        "\n"
      ],
      "metadata": {
        "id": "zGw_Mz7e2qI-"
      }
    }
  ]
}