{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8l/kabYKHkGo40xwADthZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 13: Standardization and the Parametric G-Formula"
      ],
      "metadata": {
        "id": "QgzKaDAcDCqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization was introduced in Chapter 2, but was only described as a nonparametric method. Now, we describe the use of models together with standardization which will allow us to tackle high-dimensional problems with many covariates and nondichotomous treatments.\n",
        "\n",
        "In practice, investigators will often have a choice between IPTW and standardization. Both methods are based on the same identifiability conditions but differ on modeling assumptions."
      ],
      "metadata": {
        "id": "LZvVxlunDCtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.1 Standardization as an alternative to IP weighting."
      ],
      "metadata": {
        "id": "2_NJGvWWDCvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say we wish to compute the causal difference\n",
        "$$E[Y^{a=1,c=0}]-E[Y^{a=0,c=0}]$$\n",
        "\n",
        "We also know that the associational difference\n",
        "$$E[Y|A=1,C=0]-E[Y|A=0,C=0]$$\n",
        "\n",
        "does not consistently estimate the causal effect. Assuming that consistency and positivity is not a concern, we need to achieve conditional exchangeability given $L$, and we also assume that $L$ is sufficient.\n",
        "\n",
        "Then, one way to adjust for variables $L$ is IP weighting, which creates a pseudo-population in which the distribution of the variables in $L$ is the same in the treated and in the untreated. Then, under the assumptions of exchangeability and positivity given $L$, we estimate $E[Y^{a,c=0}]$ by simply computing $\\hat E[Y|A=a,C=0]$ as the average outcome in the pseudo-population. If $A$ were a continuous treatment, we would also need a structural model to estimate $E[Y|A,C=0]$ in the pseudo-population for all possible values of $A$.\n",
        "\n",
        "An alternative to IP weighting is standardization. To compute the standardized mean outcome in the uncensored treated, we first need to compute the mean outcomes in the uncensored treated in each stratum $l$ of the confounders $L$ (i.e., the conditional means $E[Y|A=1,C=0,L=l]$ in each of the strata $l$).\n",
        "\n",
        "Then, the standardized mean in the uncensored treated is the weighted average of these conditional means using as weights the prevalence of each value $l$ in the study population (i.e., $Pr[L=l])$. More compactly, the standardized mean in the uncensored who received treatment level $a$ is:\n",
        "$$\\sum_lE[Y|A=a,C=0,L=l]\\times Pr[L=l]$$\n",
        "\n",
        "The next two sections will describe how to estimate the conditional means of $Y$ and the distribution of confounders $L$ which are the two quantities you need to estimate the standardized mean.\n"
      ],
      "metadata": {
        "id": "5HzIlHnJDCyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.2 Estimating the mean outcome via modeling"
      ],
      "metadata": {
        "id": "FJVfNKCNDC0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nonparametric estimation of $E[Y|A=1,C=0,L=l]$ is out of the question when we have high-dimensional data, which is why we often resort to modeling, such as linear regression (which imposes the linearity assumption)\n",
        "\n",
        "Also, the standardized mean should really be written as an integral because some of the variables in $L$ are essentially continuous, thus they cannot be represented by a probability function:\n",
        "$$\\int E[Y|A=a,C=0,L=l]dF_L(l)$$\n",
        "where $F_L(\\cdot)$ is the joint CDF of the r.v. in $L$.\n",
        "\n",
        "Next is standardizing these means to the distribution of the confounders $L$ for all values $l$."
      ],
      "metadata": {
        "id": "hVgLUIgwrI38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.3 Standardizing the mean outcome to the confounder distribution"
      ],
      "metadata": {
        "id": "QpgHuZkyrI6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In smaller settings, calculating $Pr[L=l]$ is trivial by calculating it nonparametrically. However, it is tedious for high-dimensional data.\n",
        "\n",
        "Fortunately, we do not need to estimate $Pr[L=l]$. We only need to estimate $E[Y|A=a,C=0,L=l]$ for the $l$ value of each individual $i$ in the study and then compute the average\n",
        "$$\\frac{1}{n}\\sum_{i=1}^{n}\\hat E[Y|A=a,C=0,L_i]$$\n",
        "where $n$ is the # of individuals in the study.\n",
        "\n",
        "This is so because the weighted mean\n",
        "$$\\sum_lE[Y|A=a,C=0,L=l]\\times Pr[L=l]$$\n",
        "\n",
        "can also be written as the double expectation\n",
        "$$E[E[Y|A=a,C=0,L]]$$\n",
        "\n",
        "The way we do this is the following:\n",
        "1. Expansion of the dataset\n",
        "2. outcome modeling\n",
        "3. prediction\n",
        "4. standardization\n",
        "\n",
        "expansion of the dataset:\n",
        "- assuming a dichotomous treatment, split the data into 3, where the first is just the original dataset and the other 2 are ones where the $L$ is same but $A$ are all 0 for data set 2 and all 1 for data set 3. For 2 and 3, $Y$ is stripped.\n",
        "\n",
        "outcome modeling:\n",
        "- use first data to model the outcome given $A$ and $L$\n",
        "- add product term $A\\times L$ to make the model saturated.\n",
        "\n",
        ">A model is saturated if it includes all possible interactions.\n",
        "\n",
        "prediction:\n",
        "- use parameter estimates from first data to predict outcome values for all rows in second and third blocks.\n",
        "- These predicted outcome values for the second block are the mean estimates for each estimation of values of $L$ and $A=0$ and for block 3 it is $L$ and $A=1$.\n",
        "\n",
        "standardization:\n",
        "- compute the average of all predicted values in both data 2 and dataset 3.\n",
        "\n",
        "The above procedure yields exactly the same estimates of the standardized means as a direct calculation. Instead of directly estimating the distribution of $L$, we averaged over the observed values of $L$, i.e., its empirical distribution. This is the way to go in more realistic examples with high-dimensional $L$.\n",
        "\n"
      ],
      "metadata": {
        "id": "apm1YWWErI8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.4 IP weighting or standardization?"
      ],
      "metadata": {
        "id": "dIkOXKwPtjW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IP weighting and standardization is only equivalent when no models are used to estimate them. This is because the quantities needed to be estimated differs between the two.\n",
        "\n",
        "IP weighting models $Pr[A=a,C=0|L]$ which we used parametric logistic regression for $Pr[A=a|L]$ and $Pr[C=0|A=a,L]$.\n",
        "\n",
        "Standardization models the conditional means $E[Y|A=a,C=0,L=l]$ which was done using a parametric linear regression model (in the book example).\n",
        "\n",
        "Model misspecification will introduce some bias and the misspecification of the treatment model (IPW) and the outcome model (standardization) will not generally result in the same magnitude and direction of bias in the effect estimate.\n",
        "\n",
        "Large differences between the two will alert to the presence of serious model misspecification in at least one of the estimates.\n",
        "\n",
        "Both IP weighting and standardization are estimators of the $g$-formula, a general method of causal inference first described in 1986.\n",
        "\n",
        "We say that standardization is a *plug-in g-formula* estimator because it simply replaces the conditional mean outcome in the g-formula by its estimates.\n",
        "\n",
        "In addition, there is no need to choose between IPTW or standardization. We can just use both via the doubly robust estimator, which, under exchangeability and positivity given $L$, will consistently estimate the average causal effect if either the model for the treatment or the model for the outcome is correct, without knowing which of the two models is the correct one."
      ],
      "metadata": {
        "id": "qphzPrNwtjZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.5 How seriously do we take our estimates?"
      ],
      "metadata": {
        "id": "XWNQNKT-5u7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validity of our causal inferences requires the following conditions:\n",
        "- exchangeability\n",
        "- positivity\n",
        "- consistency\n",
        "- no measurement error\n",
        "- no model misspecification\n",
        "\n",
        "A healthy skepticism of causal inferences drawn from observational data is necessary. To be productive, this skepticism needs to be grounded on expert knowledge about the validity of our assumptions."
      ],
      "metadata": {
        "id": "YWKZ4XLY5u-B"
      }
    }
  ]
}